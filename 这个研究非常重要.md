# Language models generalize beyond natural proteins
## 摘要
从序列中学习蛋白质的设计模式
跨进化学习蛋白质的设计模式可能对生成性蛋白质设计有前景。然而，目前尚不清楚仅基于自然蛋白质序列训练的语言模型是否能够超越对现有蛋白质家族的记忆。在这里，我们展示了语言模型不仅能泛化到自然蛋白质之外，还能生成全新的蛋白质。我们关注两个蛋白质设计任务：固定骨架设计，其中结构是指定的，以及不受约束的生成，其中结构由模型采样。值得注意的是，尽管这些模型只训练了序列，我们发现它们能够设计结构。共有228个生成的蛋白质在实验中进行了评估，以高成功率（152/228或67%）产生了可溶性和单体物种，通过大小排阻色谱法检测。在152个实验成功的设计中，有35个与已知自然蛋白质没有显著的序列匹配。在剩余的117个中，与最近序列匹配的序列同一性中位数为27%，6个设计低于20%，3个设计低至18%。对于固定骨架设计，语言模型为每个经过实验评估的人工创建的固定骨架目标生成成功的设计。对于不受约束的生成，采样的蛋白质涵盖了多样的拓扑结构和次级结构组成，并具有高实验成功率（71/129或55%）。这些设计反映了连接序列和结构的深层模式，包括在相关自然结构中出现的基序，以及在已知蛋白质家族的类似结构环境中未观察到的基序。这些结果表明，虽然语言模型仅在序列上进行训练，但它们学习了一种深层语法，使得能够设计蛋白质结构，超越自然蛋白质。  
## 引言
用于生物学的生成性人工智能有潜力开启超越自然蛋白质的蛋白质设计空间。由于氨基酸序列是蛋白质的基本编码，学习用语言模型读写这些编码可能具有前景。语言模型在近期人工智能的进展中发挥了核心作用（1），包括在复杂推理、数学问题解决、图像生成和自然语言生成（2-4）等领域的发展。性能与用于训练模型的计算能力、数据量和参数数量之间的关联由缩放定律描述（5），并且随着规模的增加，观察到更高级别能力的出现（6）。在生物学领域，最近关于蛋白质的进化规模语言模型的研究表明，从训练蛋白质序列中获得对生物内在属性的深入知识（7）。模型内部形成了关于蛋白质折叠的三维结构的信息，甚至扩展到原子级分辨率结构（8）。这些信息仅通过序列训练而获得。同时，结果显示，由序列训练得出的结构信息取决于可用的进化信息，随着训练数据中相关蛋白质数量的变化而变化（8, 9）。在各个领域中，语言模型能在多大程度上推广到其训练数据之外，这仍是一个开放性问题。在生物学中，尚不清楚语言模型是否可以用来探索超出自然蛋白质的设计空间。  

在这里，我们展示了语言模型能够推广到自然蛋白质之外，生成在序列和结构上不同于自然蛋白质的新蛋白质。我们通过实验验证了多种多样的拓扑和序列的大量设计。我们发现，尽管语言模型仅在蛋白质序列上进行训练，它们能够设计蛋白质结构，包括与自然蛋白质不同的人工设计的新蛋白质的结构。在以新蛋白质结构的骨架为目标的情况下，语言模型生成被预测会折叠到指定结构的序列。当序列和结构都是自由的时候，语言模型产生的设计覆盖了广泛的折叠拓扑和次级结构组成，创造了与自然序列分布重叠并且超越它的蛋白质。在抽样蛋白质空间中，设计在实验上成功，包括许多与自然蛋白质序列相距较远的设计。该模型生成连接序列与结构设计的基序，并能将它们应用于新的序列和结构环境中，包括复杂的氢键网络等在序列或结构上类似的已知蛋白质中未发现的基序。整体实验成功率很高，共有228个经过实验评估的蛋白质中有152个（67%）产生了可溶解和单体的物种，通过大小排阻色谱法（SEC）检测。高成功率也扩展到与自然蛋白质距离较远的蛋白质，其中49个经过实验评估的蛋白质中有31个（63%）成功。  

### 蛋白质序列的深层语法

我们假设蛋白质序列中存在一种深层的潜在语法，使得语言模型能够进行泛化。为了超越自然蛋白质，语言模型将需要访问扩展到自然蛋白质空间之外的设计模式。传统上，这种形式的泛化是由一个基于物理的能量函数启用的，该函数能够捕捉到天然折叠状态（10）。最近，基于结构的深度学习方法被提出作为通过反转结构预测（11，12）或基于主链结构的条件（13-15）来解决这个问题的新方法。通过在训练期间显式地对结构进行建模，新的深度学习方法可能会捕捉到类似于物理能量的东西（16）。语言模型在这个问题上的成功表明，序列中的深层模式可能提供一条替代的泛化路径，这条路径独立于对潜在物理模型的显式建模。  

从序列进行进化推断的经典视角是，关于蛋白质的属性信息通过保守性和共进化编码到进化相关蛋白质的序列模式中。这一观点来自于蛋白质家族的统计数据反映了对序列演化的限制，包括生物结构和功能（17，18）。这一洞察为从蛋白质家族的序列中推断结构和功能奠定了基础（19），并且最近也通过生成模型成功地用于从现有蛋白质家族生成新的例子（20-22）。到目前为止，基于序列的蛋白质设计模型的实验验证仅限于自然蛋白质家族。  

访问远离自然发生的蛋白质家族的新蛋白质设计空间是一个根本上更具挑战性的问题。根据定义，这个问题不能通过生成自然发生的蛋白质家族的新样本来解决。要用基于序列的模型解决这个问题，将需要学习超越个别蛋白质家族的序列模式。进化尺度的语言模型通过在演化过程中对多样化的序列进行训练，超越了经典的蛋白质家族模型，这意味着它们有潜力学习所有蛋白质的深层模式，包括那些没有实验结构的情况。有证据表明，序列中的局部模式超越了个别蛋白质家族，在序列中呈局部（23）以及在三维空间中呈局部的基序形式（24）。然而，序列与结构之间的映射并非一一对应（25），设计序列以达到良好折叠的天然状态需要解决一个指数级的大组合问题，以选择一组局部序列模式，这些模式非局部地相互作用以指定一个连贯的结构（26）。要设计蛋白质结构，语言模型将必须发展对序列如何决定结构的隐含理解，包括连接结构设计与序列的局部规则，以及决定序列是否连贯并将折叠成天然状态的全局规则。  

### 使用语言模型的生成性蛋白质设计
我们评估语言模型的生成能力，专注于超越自然蛋白质的泛化。由演化采样的已知蛋白质序列仅代表了庞大的可能蛋白质数量的一小部分（图1A）。要在演化探索过的蛋白质空间之外泛化，将需要访问适用于该空间之外的蛋白质设计的深层模式。我们关注两个生成性蛋白质设计任务。第一个是固定主链设计，其目标是生成一个折叠到目标结构的序列。这个任务评估了语言模型的能力，该模型仅在序列上进行训练，以设计蛋白质结构。第二个任务是自由生成，其中结构不受约束并允许随序列变化。这使得能够描述模型在不同序列和结构模式上的完整生成能力，以理解模型可访问的蛋白质空间。

![fig-1](/image/fig-1.png) 

图 1. 概览。(A) 蛋白质序列空间的示意图。自然序列（灰色）只覆盖了可能蛋白质序列的一部分。为了超越自然序列，语言模型需要访问底层设计模式。我们评估语言模型在 (i) 固定主链序列设计任务中的表现，该任务使用一组去新设计的蛋白质（绿色），以及 (ii) 通过不受限制的去新蛋白质生成任务（橙色）。(B) 语言模型 ESM2 通过掩蔽语言建模在数百万种不同的自然蛋白质上进行训练，这些蛋白质来自进化过程中的各个阶段。(C) 训练后，可以在模型的内部注意力状态中识别出有关三级结构的信息。线性投影将序列中一对位置的注意力转换为残基间距离的分布。(D) 序列的概率。模型为蛋白质中每个位置的每个氨基酸输出一个概率，这里显示的是为设计蛋白 6W3W 计算的概率。模型给表面残基的亲水氨基酸和核心中残基的疏水氨基酸赋予更高的概率。(E) 给定序列的结构概率。对于给定的序列，投影测量语言模型的内部表征与结构的兼容性。通过概率质量集中在残基间距离小于 8Å 的地方来识别三级结构。对于 6W3W，投影结构（对角线上方）与基准结构（对角线下方）之间有很好的匹配。(F) 用于生成序列的两个概率项。对于固定目标设计，我们使用 MCMC 生成序列，通过从给定结构的序列条件分布中采样。(G) 对于不受限制的生成，我们允许序列和结构都变化。(H) 使用 AlphaFold 预测的结构在单个自由生成轨迹的各个时间间隔显示。模型在缩小到一个拓扑的细化之前，抽样了一系列可能的拓扑。  

使用一组新设计的人工蛋白质测试集来评估超越自然蛋白质结构的泛化。测试集包括从蛋白质数据银行（PDB）（27）选择的多样化的结构验证的人工蛋白质结构（N = 39），涵盖一系列长度（67≤L≤184）和拓扑（图S1和附录A.1）。重要的是，这些新蛋白质在结构上与属于自然折叠的蛋白质有意义的差异，包括关于理想性、精确重复和元素的对称性。由于语言模型没有在蛋白质结构上进行训练，为这些主链生成设计测试了模型泛化到与其训练序列不同的自然蛋白质结构的能力。 

语言模型ESM2是一个进化尺度的蛋白质序列模型，已在全部自然蛋白质序列上进行了训练（28）。训练数据集排除了人工序列，以及与评估中使用的新蛋白质测试集相似的任何序列（附录A.1）。ESM2使用掩蔽语言建模目标（29）进行训练，以从序列中的上下文中恢复氨基酸的身份（图1B）。这种训练目标已被证明能够在模型的内部表示中实现关于蛋白质折叠结构的信息（7-9, 30）。由于语言模型的训练仅基于序列，因此出现的结构信息必须是对序列中模式的无监督学习的结果。 

从语言模型的注意力图中的线性投影识别出反映蛋白质结构的内部状态。之前的工作已经表明，如ESM2这样的变压器蛋白质语言模型中的特定注意力图编码了结构中残基对的接近性（9, 30）。我们进行了一个线性投影拟合，它接受蛋白质序列中两个位置之间的注意力，并输出成对距离的分布（图1C）。这将内部注意状态的660个维度映射到18个残基间距离箱中。由于参数数量有限（每个距离箱660个，总共11,898个，包括每个距离箱的偏置），远远不足以表示可能的蛋白质结构的巨大复杂性，输出可以解释为模型内部状态捕获的结构的投影。投影定义了一个能量景观（一个基于语言模型的表示状态的函数，而不是物理能量），可以用来评估任何给定结构与由语言模型产生的序列的表示的兼容性。应用于新蛋白质目标集显示了对现有新蛋白质的理解（表S1和图S2及S3）。  

综合来看，序列模型和给定序列的结构模型定义了一个由语言模型定义的蛋白质的生成模型。序列模型为任何序列分配一个概率，通过给出蛋白质中每个位置的每个氨基酸的概率（图1D）。对于自然蛋白质，这些概率反映了突变的功能效果、氨基酸的结构偏好以及生化功能的方面（31）。结构投影给出了语言模型的序列表示与三维结构之间的兼容性（图1E）。在这项工作中，我们考虑这些模型来指定一个蛋白质设计的生成模型：
p(sequence; structure) = p(structure|sequence)p(sequence)   
对于固定主链设计，蛋白质序列是通过从由语言模型指定的条件分布中进行低温采样生成的，采用马尔可夫链蒙特卡罗（MCMC）与模拟退火（图1F，附录A.3.1）。自由生成完全去除了对结构的限制，并通过从语言模型指定的序列和结构的联合分布中采样来生成新蛋白质。引入了一种阻塞的吉布斯采样方法，该方法在根据当前序列采样新结构和根据当前结构采样新序列之间交替进行（图1G，附录A.3.3）。图1H显示了一个自由生成轨迹的示例。随着温度的降低，轨迹从一个阶段开始，在这个阶段它采样了一系列可能的拓扑，然后缩小到一个单一的拓扑，该拓扑在优化的最后阶段被细化为一个自信预测的结构。  

我们对来自语言模型的总共228个设计进行了广泛的实验测试。被认为是成功的设计是那些表达良好、可溶并通过分子（流体动力学）半径的大小排阻色谱法（SEC）测试的（附录A.7）。大量生成的蛋白质的实验成功，以及对结构的独立计算评估，证明了语言模型能够访问超出自然蛋白质的设计空间。  

### 语言模型设计可折叠为新结构的序列

固定主链设计评估生成序列以实现特定目标结构的能力。使用新设计的结构作为目标要求模型超越自然蛋白质，需要使用更一般的结构设计模式。此任务的成功将表明该模型理解了蛋白质结构的基本设计原则，并能泛化到未被自然序列编码的结构。   

在39个人工设计的新蛋白质结构的测试集中，由语言模型生成的固定主链设计被AlphaFold高分辨率结构预测模型预测与目标结构高度匹配。我们为每个新目标结构生成了200种不同的设计（附录A.4）。生成模型成功产生了绝大多数新测试集目标的低RMSD设计（图2A）。从语言模型的优化目标中选取最佳的200个设计中的前10个，84%（33/39）的目标中位RMSD < 2.5埃，90%（35/39）的目标最小RMSD < 2埃。结构也被自信地预测，56%（22/39）的目标中位pTM > 0.7，最大pTM > 0.7为90%（35/39）。与目标的平均序列同一性低（22%），表明语言模型找到了与原始序列不同的设计问题的解决方案。 

![fig-2](/image/fig-2.png) 

图 2. 为去新结构设计序列。(A) 使用计算机模拟的预言者对去新目标集的设计进行总体评估。根据每个目标的优化目标，为前 10 个设计绘制了设计结构（预言者预测）与目标结构之间的 C-α 原子的均方根偏差（RMSD）。目标按长度递增排序。语言模型生成的序列预测能够折叠到测试集中大多数去新主链的目标结构（39/33 达到中值 RMSD < 2.5Å）。(B) ESM 设计的实验结果。根据包括序列新颖性和手动检查有趣基序在内的多种标准，选择了 6 个去新主链目标的 79 个设计。如果设计可溶，并且通过凝胶过滤色谱（SEC）在预期的洗脱体积处有一个峰，则认为设计成功。当唯一的峰出现在预期洗脱体积时，设计被归类为单分散。总体而言，78% 成功，39% 为单分散。(C) 使用和不使用语言模型的设计的实验结果比较。对于四个目标中的每一个，根据优化目标从 200 个设计中选择前 5 个进行实验评估。总体而言，使用语言模型的设计成功率为 95%，而大多数不使用语言模型的设计因不溶而失败。(D)（左）优化轨迹显示语言模型指定的能量与 MCMC 优化过程中的 RMSD 到目标的关系。能量降低并汇集到低 RMSD。（右）在每个轨迹结束时，根据能量选择的前 5 个设计的可视化。(E) 设计的语言建模困惑度。语言模型的设计被语言模型视为可能，而基线设计的高困惑度表明它们的序列被视为不可能。这与实验成功相吻合。(F) 使用和不使用语言模型的设计的 SEC 迹线比较。绝大多数使用语言模型的设计是可溶的，并且在预期的洗脱体积处有一个峰；相比之下，很少有不使用语言模型的设计是可溶的。(G) 一部分额外成功的语言模型设计相对于已知自然蛋白质是新颖的。显示了四个不同主链的示例，设计叠加在对自然蛋白质进行序列搜索后的最高显著性命中的预测结构上。在每种情况下，最接近的自然序列具有低序列同一性（<0.3）和具有不同拓扑的预测结构。  

在实验室中生成的蛋白质具有高整体实验成功率。我们进行了一组额外的固定主链设计轨迹，以探索模型生成的设计基序的多样性。从包括额外轨迹的池中选择了涵盖6个新目标的79个固定主链设计，通过包括有趣的结构基序的存在等各种标准进行评估（附录A.6）。在这组实验测试的蛋白质中，97%（77/79）可溶，78%（62/79）通过大小排除色谱法（SEC）测试成功，表明在预期洗脱体积处有峰值，指示折叠的单体物种，39%（31/79）单分散，仅在预期洗脱体积处显示单个SEC峰（图2B）。成功案例涵盖了多种拓扑，包括长度为182的新TIM-barrel 6WVS的成功案例，该案例具有高度理想化的对称结构（图S4）。在实验成功的集合中，与目标结构的原始序列的序列同一性低（平均值=24%），这表明语言模型正在探索目标结构的新设计空间。    

我们进行了对照实验，以了解语言模型在设计成功中的作用。作为比较，我们使用AlphaFold作为给定序列的结构概率的模型。对于具有不同折叠的四个固定主链新目标，我们使用每种方法生成了200个设计，每种方法的前5个按优化目标选出进行实验评估（附录A.3）。实验显示，95%（19/20）的语言模型序列设计和5%（1/20）未使用语言模型的设计成功（图2C）。使用n-gram先验增强AlphaFold未能挽救设计（成功率0%，0/20）（表S3和S4）。      

语言模型困惑度在两种设计方法中区分了成功与失败。语言模型的MCMC轨迹随着能量降低而聚焦到低RMSD，平均RMSD值从1.1埃到2.4埃不等（图2D）。值得注意的是，虽然AlphaFold自信地预测了语言模型设计的结构，但语言模型并未给AlphaFold的设计分配高序列可能性。选定的AlphaFold设计序列的语言模型困惑度从10.6到13.1不等（图2E），显著高于平均新目标序列困惑度6.7。其他度量标准在识别实验成功方面的能力有限（图S5和表S4）：Rosetta全原子能量函数用于建模和设计（32，33）认为两组都是好设计，封装度量相似但略微偏向（未成功的）AlphaFold设计，而疏水性和SAP评分偏向语言模型设计。最近，直接基于目标结构的自回归逆向折叠模型在实验室中展示了高实验成功率（15）。我们使用ProteinMPNN和ESM-IF1（14）生成序列。这两个模型都达到了高局部置信度pLDDT（>90平均）。它们的ESM伪困惑度分别为5.76和5.79，高于ESM设计并显著低于AlphaFold设计（表S2），与这些方法报告的高实验成功率一致。    

对两组设计（使用和不使用语言模型）的实验评估表明，19/20的语言模型设计成功，9/20为单体（图2F）。6D0T目标没有来自语言模型的单体设计，尽管作为阳性对照测试的新序列也被发现不是单体（附录A.7）。未使用语言模型的设计主要因不溶而失败。    

包括对照比较的结果和评估的更大设计集，语言模型为所有8个新主链成功生成了实验成功的设计。一种可能性是，语言模型设计之所以成功，是因为模型从其训练集中检索到与目标相似的蛋白质。为了排除这种可能性，我们分析了81个实验成功的总体设计。每个设计都在UniRef90中进行搜索（完全包括用于训练语言模型的序列），以识别相似的序列（附录A.5）。在17个成功的设计中，涵盖4个主链的设计中没有任何显著的（E值<1）序列匹配，在训练集中。其中四个显示在图2G中。其余64个中，与最近序列匹配的序列同一性平均只有27%，且有41/64的同一性<30%，涵盖了8个测试的主链中的每一个。这表明模型并非通过检索其已记忆的相似序列来解决设计问题。    

为了进一步了解模型是否利用同源性来检测序列相似度的阈值，我们获得了 AlphaFold 预测的结构，包括那些未达到显著性截止的结果（附录 A.5；图 S6）。在 81 个实验成功中，有 19 个的顶级 Jackhmmer 命中结果并非设计的结构匹配。对于涵盖 4 个主干的 19 个设计，顶级 10 个 Jackhmmer 命中（包括那些未达到显著阈值的）的 TM 评分均 < 0.6。在这些设计中，有 8 个涵盖相同的 4 个主干，顶级 10 个命中结果很可能是不同的折叠（TM 评分 < 0.5）。这表明虽然在某些情况下模型能够利用序列同源性进行检测，但也有情况显示它似乎已经在此基础上进行了泛化，这进一步证明在许多情况下，语言模型正在为设计问题生成新颖的解决方案，这些解决方案与基准序列和自然蛋白质都不同。  

语言模型实现了蛋白质设计的深层模式。生成的蛋白质显示出利用蛋白质结构设计的深层模式的证据。这些模式以自然蛋白质设计中使用的结构基序的形式出现，在显著不同的序列环境中应用，以及形成在相关结构中找不到的基序。序列决定结构的两种研究良好的方式是通过限制脊椎几何的氨基酸，以及通过化学多样的侧链在确定稳定蛋白特定折叠构象的分子间力中的作用。影响脊椎几何的两种氨基酸是脯氨酸和甘氨酸。这两种氨基酸分别增加蛋白质主链的灵活性和弯曲。在三个示例设计中，语言模型将这些残基放置以在各种二级结构元素中诱导曲率：一个脯氨酸使 alpha 螺旋弯曲，规则放置的甘氨酸在 beta 片中促进形成 beta 桶的灵活性，而除一个甘氨酸外，所有甘氨酸都放在 NTF2 设计的环中（图 3A）。一种基于侧链的基序是通过固定主链设计出现的 alpha 螺旋偶极盖，其中 alpha 螺旋末端的氨基酸侧链掩盖了最终 alpha 螺旋转中原本暴露的极性主链原子（图 3B）。第二种基于侧链的基序是含隆起的 beta 转中的氢键网络，这在固定主链的 beta 桶设计中存在，如 6D0T 和 6CZJ（图 3C）。此外，图 3A 中 beta 条的周期性甘氨酸被识别为自然基序，使得目标 beta 桶的去新设计成功。  

![fig-3](/image/fig-3.png) 

图 3. 语言模型具体化了蛋白质设计的深层模式，生成类似天然的和去新的基序。(A) 在三种不同设计的蛋白质中放置脯氨酸或甘氨酸，使 alpha 螺旋、beta 片和转角产生曲率。(B, C) 转角中的氢键网络。(B) 螺旋偶极盖通过形成氢键来遮蔽最终螺旋转中的极性主链原子。(C) 在涉及 beta 片的转角中形成的氢键网络。(D, E) 设计蛋白和天然蛋白中基序的比较。设计（左）与通过序列搜索（中）和结构搜索（右）找到的天然蛋白质中最近的基序进行比较。命中结果按基序位置的匹配氨基酸排序。(D) 一个设计中使用的氢键基序的示例。找到的序列匹配具有相同的基序在对齐的位置。然而，周围的序列环境显著不同，序列同一性为 26%。(E) 可能的去新氢键网络的示例。不仅序列环境不同，基序本身在任何匹配的自然序列或结构的对齐位置中也不存在。  

设计还展示了复杂的氢键网络。一些设计成功包括四个或更多极性甚至带电残基在结构内部的氢键网络。由于能量满足这些相互作用的几何约束，设计埋藏的极性和带电相互作用是困难的。值得注意的是，所显示的键网络跨越了一系列分子间力类别：在预测结构中，F129（一个 beta 桶）含有一个盐桥，F025 含有一个 pi-阳离子键，而 F030 含有一个 T 形的 pi-pi 相互作用（图 S7）。所示例的原始设计具有纯疏水内部。虽然这些氢键网络只有通过高分辨率结构研究才能完全确认，但观察到的生物物理特性（高产率的单分散蛋白和预期的保留体积）与它们的准确性一致，因为这些残基的不准确放置可能导致错误折叠和聚集。  

这些含极性残基的氢键网络在新的序列环境中实现，表明了一种强烈的泛化形式，超出了用于训练模型的序列。我们通过 Jackhmmer 搜索 UniRef90 和类似的方式检索最相似的对齐序列，通过 Foldseek 搜索 AlphaFold DB（37）检索相似的对齐结构。返回的序列都按对齐基序位置的最小编辑距离排序，显示最匹配的基序（附录 A.5.4）。对于生成的蛋白质 F030（图 3D，图 S7），序列搜索确实恢复了一个含有此基序的自然蛋白质在对齐位置。然而，设计中的周围序列环境不相似，全序列同一性为 26%。对于 F129 和 F092（图 3E，图 S7），不仅周围的序列环境同一性低，基序本身在任何匹配的自然序列或结构的对齐位置中也不存在。在固定主链设计中使用这些基序是一种显著的泛化形式，因为模型在新的序列环境和与自然蛋白质不同的结构中应用它们。  

### 语言模型生成新颖的结构和序列

语言模型生成与自然序列显著不同的新蛋白序列。我们抽取了一个大型固定长度（L = 100）蛋白集合（N = 25,000），不受结构约束。通过联合序列和结构的景观进行遍历的阻塞 Gibbs 采样方法提供了比以前的无约束生成方法更多样化的蛋白集合（表 S5）。　　

生成物涵盖了多种拓扑结构，其序列与自然蛋白质大不相同。所有生成的序列都使用 Alphafold 进行结构预测，并使用基于它们的成对结构距离（通过 TM 评分测量）的 t-SNE 投影到二维空间（图 4A）。在结构的层次聚类中，识别出 7,663 个不同的簇，TM 评分阈值为 0.75。生成的次级结构的分布揭示了一系列模式，其中 52% 的生成物主要包含 alpha 螺旋，22% 主要包含 beta 片，28% 是 alpha 螺旋和 beta 片的混合（图 4B）。大部分生成物通过预言者（oracle）的预测效果很好（中位 pLDDT = 84.49，70% pLDDT > 70；图 4C）。　

![fig-4](/image/fig-4.png) 

图 4. 语言模型生成新颖的结构和序列。(A) 使用 t-SNE 嵌入由生成的蛋白质覆盖的结构空间。颜色表示与最佳匹配的天然序列的序列同一性。这部分空间的大部分具有较低的序列同一性，16% 的生成物没有与天然蛋白质的显著序列匹配。在实验评估中成功的设计用绿色星星标示。(B) 生成物的次级结构分布。实验成功（绿色）在不同次级结构的组合中观察到。(C) pLDDT 和 pTM 的分布表明设计通过计算机模拟预言者的预测较好（中位 pLDDT 为 84.5）。(D) 序列和结构与天然蛋白质的相似性密度图。对于每个生成的蛋白质，从 AlphaFoldDB 检索最匹配的天然序列。每个生成的蛋白质根据其序列相似性（x 轴）和结构相似性（y 轴）绘制，未通过显著性阈值的命中在 x 轴上标为零。生成的蛋白质占据与天然蛋白质不同的空间部分，其中一部分与天然蛋白质的序列相似性最小（左下象限）。通过计算机模拟筛选和实验成功的设计与生成物的整体分布相重叠。(E) 实验评估的总体结果。测试的设计中大多数（55%）通过了溶解性测试，并且洗脱体积峰位于正确的置信区间（顶部）。此外，远离天然序列的评估蛋白质中有很高的比例（63%）成功（底部）。(F) 六个实验成功的预测结构（顶部）。结构与来自天然蛋白质序列搜索的其最显著命中的预言者预测结构对齐（底部）；在所有示例中，预测的拓扑结构都不同。(G) 对于 F 面板中的生成物，显示了与图 3A - 3C 中相同的基序：脯氨酸和甘氨酸引起的曲率，螺旋盖，以及转角中的氢键网络。即使在与天然蛋白质相似性最小的蛋白质上，语言模型也能产生已知的基序。  

许多生成物在序列上与自然蛋白质相距甚远。我们通过将每个生成物与 AlphaFold DB（37）中的 2 亿自然序列进行搜索，测量生成序列与自然蛋白质的距离。这还使得比较最接近序列匹配的结构与生成蛋白的结构成为可能。总体而言，语言模型生成的蛋白质显示出与自然蛋白质分布的明显分离，包括一部分与已知蛋白质相距甚远的蛋白质。图 4D 显示了与已知蛋白质的相似度分布，其中每个生成物根据其序列（x 轴）和结构（y 轴）与最高序列命中的相似度进行绘制，不显著的命中（E-value > 1）放在 x=0（总共 16.6% 的生成物）。生成蛋白质的大部分分布具有与其最近序列匹配预测的结构不同，进一步证明模型不仅仅是记忆已知蛋白质。还展示了 15k 自然蛋白质的集合。自然蛋白质聚集在右上角，而生成的蛋白质占据了空间的不同部分。语言模型生成的蛋白质中有显著一部分（15.5%）与自然蛋白质几乎没有相似性（左下象限），最近匹配的序列相似性很低（Seq-id < 0.2），预测结构很可能是不同的折叠（TM 评分 < 0.5）。　

其中包括与自然蛋白质相距甚远的设计，大部分在实验中成功。我们选择了一些通过我们计算机模拟质量筛选的设计进行实验评估。在所有生成物中，20%（N = 5,198）通过了质量筛选（附录 A.4）。其中 129 个被表达和评估，55%（71/129）被发现在实验中成功。这 71 个结构及其指标显示在图 S8 中，用绿色星号标记在图 4A、4B 和 4D 中。总体而言，评估的自由生成物中 96% 是可溶的，55% 在正确的置信区间内有洗脱体积峰，30% 是单分散的（图 4E 顶部，附录 A.7）。　　

对于与自然蛋白质相距甚远的生成物，也观察到了高成功率。对于 49 个相距甚远的生成物（图 4D，左下象限），有 31 个（63%）在实验评估中成功。对这 31 个实验成功，我们进行了对自然蛋白质的相似性的更深入分析。我们进一步搜索每一个对 UniRef90 进行，它提供了自然蛋白质的全面覆盖，并完全包含了语言模型的训练集。在 31 个相距甚远的设计中，有 16 个没有任何显著的（E-value < 1）序列匹配（图 S9）。我们获得了前 10 个序列匹配的预测结构，无论它们的显著性如何。在这 31 个相距甚远的设计中，有 12 个（其中 5 个显示在图 4F 中），序列匹配中没有一个可能具有相同的折叠（TM 评分 < 0.5）（图 S9）。预测的结构通常是自信的（78% 的预测 pLDDT > 70，平均 pLDDT = 81.24）。在去新生成物中观察到的结构基序，如脯氨酸和甘氨酸的放置、螺旋盖和氢键网络，也出现在固定主链设计中（图 4G）。总体而言，这些结果表明语言模型在自然蛋白质的空间之外泛化，生成去新蛋白质。　　

### 语言模型的进化尺度

由 (7) 引入的转换器蛋白语言模型发现了关于功能和三级结构的信息的出现的证据，这是通过无监督训练得到的。同时，小规模的研究检查了基于 LSTM 的模型（38–40）。现在已开源数十亿参数的大规模蛋白语言模型（8, 41–43）。近期，通过计算机模拟研究（44, 45）和实验确认为现有蛋白家族生成新序列的功能，已探索了语言模型的生成使用。据我们所知，基于序列的模型的实验验证工作（20, 22, 46）尚未突破与自然蛋白质 < 30% 的同一性阈值。  

## 结论

传统上认为序列空间由每个蛋白质家族周围的独立局部进化景观构成，这表明语言模型可能仅限于记忆自然蛋白质的空间。与此一致的是，蛋白质语言模型中出现的关于结构的信息已被证明依赖于模型在训练期间可用的进化信息，这对于使用语言模型在自然蛋白质之外生成性地使用的潜力似乎并不乐观。然而，我们在这里呈现的证据与此相反：语言模型在自然蛋白质家族之外泛化，生成位于自然蛋白质远端的序列空间中的蛋白质。我们的结果是第一次纯粹基于序列的方法被证明可以超越自然蛋白质泛化，并且对于基于序列的生成性人工智能进行去新蛋白质设计非常有前景，我们已经证明存在一个去新蛋白质的空间，这些蛋白质远离自然中的蛋白质，可以通过生成性语言模型设计。  

这种泛化指向了自然序列背后的更深层结构，以及语言模型可学习的深层语法的存在。我们的结果表明，通过进化创造的大量蛋白质序列包含了生物结构和功能的图像，这揭示了适用于各种蛋白质的设计模式，这些模式可以被完全基于序列的模型学习和重新组合。超越自然蛋白质的泛化并不一定表明语言模型正在学习物理能量。语言模型可能仍在学习模式，而不是物理能量，但可以推测，在无限序列数据的极限情况下，这些模式可能会逼近物理能量。至少，语言模型必须已经发展出对蛋白质的全局一致性的理解，连接序列和折叠结构。  

蛋白质之间存在深层语法可以解释两个表面上似乎矛盾的观察结果：对自然蛋白质的理解依赖于训练数据中的进化支持，同时语言模型也在已知的自然蛋白质家族之外泛化。如果可学习模式存在幂律分布，那么预期许多蛋白质结构将能够使用训练数据中得到最多支持的常见模式来设计。同时，模式在训练数据中观察到的频率将与模式的可学性相对应。要学习罕见模式，需要更多的训练数据和模型容量。这与观察到的泛化到新设计空间（通过已学习的模式可访问）和依赖训练数据支持（由罕见模式组成的蛋白质更难学习）的现象一致。如果蛋白质语言模型的规模法则继续成立，我们可以期待它们的生成能力将继续提高。随着模型和数据的规模扩大，可学习的底层语法的存在将预示着将学习罕见模式，扩展模型的预测能力和可访问的设计空间。  

## A. 方法

### A.1. 数据

#### A.1.1. 去新目标集
一组去新蛋白质被用于固定目标主链的设计任务。从蛋白质数据银行（Protein Data Bank, PDB）（27）中选出了一组多样化的去新结构（N = 39），这些结构涵盖了不同的长度（67 ≤ L ≤ 184）、折叠类型（例如 alpha-bundle、beta-barrel、NTF2、Rossman）和去新设计方法（26, 34, 47-56）。参见图 S1，查看去新目标集包含的所有X射线晶体结构的视觉展示。这些蛋白质是由人类设计的，而非自然进化过程。重要的是，这些去新蛋白质与属于自然折叠的蛋白质具有有意义的结构差异。例如，NTF2 目标具有非自然的结合口袋（55），beta-barrels 更窄且具有短的 beta-turns（34），一些设计在其创建时是全新的折叠（47）。尽管这些蛋白质在序列和结构上与自然蛋白质明显不同，但目标集中的每个蛋白质都针对 UniRef100（28）进行了查询，该数据库包含了 ESM2 的训练集，并且所有由 Jackhmmer 搜索返回的匹配序列都从语言模型的训练中排除，详见下一节。  

去新目标集的蛋白质数据银行识别码（PDB IDs）为：  
1QYS, 2KL8, 2KPO, 2LN3, 2LTA, 2LVB, 2N2T, 2N2U, 2N3Z,  2N76, 4KY3, 4KYZ, 5CW9, 5KPE, 5KPH, 5L33, 5TPJ, 5TRV,  
6CZG, 6CZH, 6CZI, 6CZJ, 6D0T, 6DG6, 6DKM A, 6DKM B, 6DLM A, 6DLM B, 6E5C, 6LLQ, 6MRR, 6MRS, 6MSP, 6NUK,  6W3F, 6W3W, 6WI5, 6WVS, 7MCD。    

#### A.1.2. 用于训练 ESM2 的序列数据集
在整个工作中使用的语言模型是 ESM2 650M (8)。因此，该工作中描述的所有预训练设置适用于此处使用的语言模型。  

为了测试语言模型对蛋白质的理解是否从自然到去新空间泛化，至关重要的是，模型在训练时没有看到去新蛋白质。为此，我们首先从 ESM2 的训练集中移除所有在 UniProt（57）网站上标记为“人工构建”的序列，当2021年04月是最新发布时（总共1,027个蛋白质）。为了防范标签错误的蛋白质，并进一步移除训练集中可能与目标集相似的序列，我们还进行了对每个去新序列对 UniRef100 2021 04 的 Jackhmmer（58）搜索，并移除所有由此工具返回的命中结果（58,462个蛋白质）。  

#### A.1.3. 结构投影数据集
结构投影网络在一个由15,051个蛋白质组成的非冗余PDB数据集上进行训练（结构发布日期在2018年5月1日之前），用于Yang等人的研究（59）。    

#### A.1.4. 自然蛋白质的保留集
为了在评估语言模型对去新蛋白质理解时提供基线比较，选择了一小部分（N = 214）具有PDB结构的自然蛋白质，构成图 S1 和 S2 以及表 S1 中的保留集。这组数据由2020年7月可获得的PDB构成，它们的序列同一性小于0.3，根据用于训练结构投影的数据集，通过mmseqs2（60）计算。应用了长度过滤器（50 ≤ L < 250）以大致匹配去新目标集的长度分布（67 ≤ L ≤ 184）。  

### A.2. 模型

#### A.2.1. ESM2
在本研究中，我们选择 ESM2 650M（8）作为大规模蛋白质语言模型。ESM2 是一个通过掩码语言建模训练的变压器模型，应用于已知的自然蛋白质序列。在训练期间，部分氨基酸残基被掩码、随机置换为另一种氨基酸或保持未修改，这符合标准的BERT噪声概率（7）。模型的任务是预测这些掩码残基，给定输入中所有未掩码残基的双向上下文。ESM2 仅在自然蛋白质序列上训练；被标注为人工构建的序列和通过序列搜索与去新目标集查询匹配的序列已从语言模型的训练集中移除（附录 A.1.2）。

语言模型用于通过伪似然（61）近似 p(sequence)。首先定义序列 x 中位置 i 的可能氨基酸的概率 p(x_i|x_{-i})，条件是序列的其余部分。这个条件概率是通过构建 x_{-i}（在 i 位置的氨基酸被替换为 <mask>），并计算该位置的语言模型概率得到的。然后定义伪似然为 p(x_i|x_{-i})。

#### A.2.2. 结构投影
结构投影是一个从 ESM2 内部表征到残基间距离的单一学习的仿射投影（带偏置项的线性投影），应用于蛋白质的每一对位置 [i; j]。

在其实现中，给定序列的 ESM2 推理期间计算的（N = 660）注意力图被用作线性投影的输入。在位置 [i; j]，我们计算维度为 (N = 18) 的 zij：zij = W_projection * attention_maps_ij + b_projection。向量 zij 是定义了在碳-β原子间距离的分箱分布 p(d_ij|sequence) 的 softmax logits，这种分布被称为 distogram（62）。

在条件成对独立性假设下，我们使用 ij;i≠j p(d_ij|sequence) 来近似 p(structure|sequence)。结构投影中总共学习了 11,898 个参数。模型的分箱分辨率为 1Å，有 16 个区间覆盖范围 [2.5Å, 20Å]。第一个区间表示 <2.5Å，最后一个区间表示 >20Å。由于 distograms 本质上是对称的（d_ij = d_ji），预测 logits 应用了对称性。在结构投影的训练期间，ESM2 模型的权重被冻结。

结构投影在 Yang 等人（59）发布的序列和结构对的随机子集（80%）上进行训练（附录 A.1.3）。如同该研究，从推断出的碳-β坐标构建 distograms。我们训练了 10 个周期，批大小为 4，学习率为 1e-2，使用分类交叉熵损失，比较预测的 distogram 和真实的 distogram 之间所有 [i; j] 对。用于学习结构投影的数据集与去新目标集之间没有共同的结构。

#### A.2.3. N-GRAM 先验
通过 UniRef50（2018年03月版本）中的氨基酸频率确定了单、双和三元（n-gram

）氨基酸频率的背景分布。在设计过程中，计算背景分布和设计序列的 n-gram 频率之间的 Kullback–Leibler 散度（DKL）。DKL 项被加权平均以产生单个 n-gram 能量项。从概念上讲，这也可以看作是使用 n-grams 作为语言模型 p(sequence)（63），它可以与 ESM 变压器语言模型结合。具体来说，能量函数定义在我们设计序列和背景的 n-gram 频率之间。

![image](https://github.com/pengsihua2023/esm/assets/131550223/ded34783-4d61-44db-8af1-50dbfc355643)

### A.3. 任务

#### A.3.1. 固定主链设计
固定主链设计的目标是为目标主链 y 生成一个蛋白质序列 x。如同（59）中所述，主链源自蛋白质碳-β原子的三维坐标集（对甘氨酸进行推断），其长度等于蛋白质中的残基数量。这些三维坐标被转换为分箱成对距离的 distogram（附录 A.2.2）。

我们希望在给定目标主链 Y 的条件下，采样具有高可能性的序列 x。  

![image](https://github.com/pengsihua2023/esm/assets/131550223/26d767fb-c2bc-417e-aa79-42701053bb34)
  
为了从这个分布中抽样，我们首先根据贝叶斯规则注意到，这相当于从无条件序列先验 p(x) 和条件结构分布 p(y|x) 的未归一化乘积中进行抽样：  
 ![image](https://github.com/pengsihua2023/esm/assets/131550223/f07b0f1e-8487-4a80-b31f-604ea519f80c)

• p(x)：通过语言模型的伪似然来近似，该伪似然是通过在掩盖每个单独的标记时乘以边缘似然以及 n-gram 先验来计算的。  
![image](https://github.com/pengsihua2023/esm/assets/131550223/f46ecd65-0b52-4b96-a66c-242a1d63177e)
突变为半胱氨酸是不被允许的，因为它们的存在会干扰实验评估。请注意，通过将接受率定义为 E(x0) 和 E(x) 的比率，可以在语言模型的一次前向传播中高效地近似 ELM(x0) 与 ELM(x) 之间的相对比率（通过在替换位置计算边际替换似然），这与显式计算 ELM(x) 所需的 L 次前向传播相比。    

### A.3. 任务

#### A.3.1. 固定主链设计
固定主链设计的目标是为特定的主链 y 生成蛋白质序列 x。如 (59) 所示，主链源自蛋白质碳-β原子的三维坐标集（甘氨酸推断得出），长度等于蛋白质中的残基数。这些三维坐标被转换为双残基距离的分箱 distogram（附录 A.2.2）。  

我们希望在目标主链 Y 的条件下，采样高概率序列 x。  
 
为了从这个分布中采样，我们首先根据贝叶斯规则注意到，这等同于从无条件序列先验 p(x) 和条件结构分布 p(y|x) 的未归一化乘积中进行采样：  

• p(x): 通过语言模型的伪似然来近似，该伪似然是通过在掩盖每个单独的标记时乘以边缘似然以及 n-gram 先验来计算的。  

我们进行了 170,000 步的 MCMC 采样。我们使用了模拟退火的几何衰减温度计划。每 10,000 步，我们将温度 T 降低 2，从初始值 8 降到大约 6e-5。固定主链设计的完整设计轨迹大约需要 10 小时，序列长度大约 100，在单个 32GB Volta GPU 上进行。我们可以在大多数目标上实现成功的设计（根据预言者的低目标 RMSD），但增加了步数以达到最佳性能，特别是对于较长的固定主链设计，例如 6WVS（L = 182）。  

#### A.3.2. 无语言模型的固定主链设计
使用语言模型（"LM"）的设计与使用强大的结构预测器但没有语言模型（"no-LM"）的基线进行了比较。对于这个基线，使用 AlphaFold 作为结构模型。
为了保持与 LM 设计的比较匹配，使用 AlphaFold 的双残基距离（distogram）输出作为 p(结构 = 目标|序列)。由于没有使用变压器语言模型，没有 p(序列) 项，无语言模型的固定主链设计优化序列，使其在 p(结构|序列) 中具有高概率。此外，为了确保与 LM 设计的完全匹配的比较，生成了第二组 no-LM 设计，包括用于 LM 设计的相同的 n-gram 项（附录 A.2.3）。额外的 n-gram 项可以被解释为添加了一个弱 n-gram 语言模型。这个 n-gram 项的系数是通过线性扫描选择的（表 S3）。在主要比较中，我们只展示没有 n-gram 项的组的结果，因为该组在实验中更成功（1/20 成功 vs. 0/20 带 n-gram 项的成功）。   

使用基于 AlphaFold 的公共算法生成设计。基线设计由 ColabDesign (12, 16)（提交哈希 e7bb3def）产生，使用设计 3stage() AfDesign 配方，该配方交替然后同时优化 AlphaFold 的所有 5 个 pTM 模型副本。发现增加步骤改善了优化过程中达到低目标 RMSD 的收敛，因此默认的步骤数增加了 5 倍，总共 1500 软迭代，500 温度迭代和 50 硬

迭代。这种设计协议由于使用了基于梯度的优化，所需的优化步骤较少；该算法可以在每一步更新序列中的每个位置，而我们使用的 MCMC 协议每步只进行单一突变。虽然优化了 AlphaFold 的 distogram 输出而不是其原子结构预测输出，但所有设计都被验证具有小于 1°A RMSD 至目标和大于 0.8 pTM 根据 AlphaFold Oracle（附录 A.4.1）。  

LM 和 no-LM 协议用于每个目标产生 200 个设计。简单选择每个协议优化目标的前 5/200 个种子（每个目标），用于选择进行实验评估的设计（附录 A.6.2）。  
 

